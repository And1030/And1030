{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGNpgnHazObVW3fCgnqHra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/And1030/And1030/blob/main/Entrega_prim_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a id=\"section0\"></a>\n",
        "# <font color=\"#004D7F\" size=6> Paquetes a instalar </font>"
      ],
      "metadata": {
        "id": "GN6y5zUkdpT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets\n",
        "!pip install scikeras\n",
        "!pip install seaborn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PxhYgz0HdxUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import tensorflow_datasets as tfds\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import learning_curve"
      ],
      "metadata": {
        "id": "LcnxWHIZeTAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cargar el archivo base**"
      ],
      "metadata": {
        "id": "XOHbRuFqez8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(\"/content/sample_data/databus.csv\", delimiter=\",\")\n",
        "dataframe.dropna(axis=1, inplace=True)\n",
        "dataset = dataframe.values\n",
        "print(dataset.shape)\n",
        "dataset"
      ],
      "metadata": {
        "id": "adHgxMrZezk3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividir los datos\n",
        "#X para los datos (variables) de entrada\n",
        "#Y para los datos (variables)  de salida\n",
        "X = dataset[:,0:9]\n",
        "y = dataset[:,9]\n",
        "\n",
        "# Definir KFold para validación cruzada\n",
        "kfold = KFold(n_splits=10)\n",
        "\n",
        "# Inicializar listas para almacenar errores de entrenamiento y validación\n",
        "train_errors, val_errors = [], []\n",
        "\n",
        "# Regularización L2\n",
        "l2_regularizer = tf.keras.regularizers.l2(0.01)  # ajustar la fuerza de regularización según sea necesario\n",
        "\n",
        "epochs,epoch=200,200\n",
        "batch_size=10"
      ],
      "metadata": {
        "id": "UV2u01WbOX1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Crear el modelo**"
      ],
      "metadata": {
        "id": "3HJJBf8Zfbxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate_schedule(epoch):  # Definir explícitamente el argumento de la época.\n",
        "  \"\"\"\n",
        "  Función que decae la tasa de aprendizaje con cada época.\n",
        "  \"\"\"\n",
        "  initial_learning_rate = 0.01  # Ajuste este valor según sea necesario\n",
        "  decay_rate = 0.95  # Ajuste este valor según sea necesario (entre 0 y 1)\n",
        "  decay_steps = 10  # Ajuste este valor para controlar la frecuencia de caída.\n",
        "  return initial_learning_rate * decay_rate**(epoch / decay_steps)"
      ],
      "metadata": {
        "id": "AyfuYAHlprSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wider_model():\n",
        "  # crear modelo\n",
        "  model=Sequential()\n",
        "  model.add(Dense(30, input_dim=9, activation='relu', kernel_regularizer=l2_regularizer))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(60, activation='relu', kernel_regularizer=l2_regularizer))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(46, activation='relu', kernel_regularizer=l2_regularizer))\n",
        "  model.add(Dense(1))\n",
        "  # Compilar modelo con programa de tasa de aprendizaje\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule(epoch)),\n",
        "                metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "  return model\n",
        "model = wider_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EWetShykfa8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluar el modelo**"
      ],
      "metadata": {
        "id": "8fwCvhb5gUVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluar el modelo con un conjunto de datos estandarizado\n",
        "estimators=[]\n",
        "estimators = [\n",
        "    ('standarize', StandardScaler()),\n",
        "    ('mlp', KerasRegressor(model=wider_model,\n",
        "                           epochs=epochs,\n",
        "                           batch_size=batch_size,\n",
        "                           verbose=0))\n",
        "]\n",
        "pipeline = Pipeline(estimators)\n",
        "results=cross_val_score(pipeline,X,y,cv=kfold)\n",
        "print(\"Deep NN: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tJnlemplgU7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validacion del modelo**"
      ],
      "metadata": {
        "id": "jJgCzTDPC1tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir una lista de épocas sobre las que iterar\n",
        "epochs_list = range(1, 601, 1)  # Rango de 10 a 300 con un paso de 10\n",
        "\n",
        "# Inicializar diccionarios para almacenar errores de entrenamiento y validación para cada época\n",
        "errors = {}\n",
        "loss_history = {}  # New dictionary to store loss history\n",
        "\n",
        "for epochs in epochs_list:\n",
        "  train_errors, val_errors, train_losses, val_losses = [], [], [], []\n",
        "\n",
        "  # Definir pipeline\n",
        "  estimators = [('standarize', StandardScaler()),\n",
        "                ('mlp', KerasRegressor(model=wider_model,\n",
        "                                       epochs=epochs,\n",
        "                                       batch_size=batch_size,\n",
        "                                       verbose=0))]\n",
        "\n",
        "  pipeline = Pipeline(estimators)\n",
        "\n",
        "  # Cross-validation loop\n",
        "  for train_index, val_index in kfold.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Ajustar el Pipeline a los datos de entrenamiento.\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # predicciones tanto en conjuntos de entrenamiento como de validación.\n",
        "    y_pred_train = pipeline.predict(X_train)\n",
        "    y_pred_val = pipeline.predict(X_val)\n",
        "\n",
        "    # Calcular errores de entrenamiento y validación (asumiendo MSE)\n",
        "    train_error = np.mean((y_train - y_pred_train) ** 2)\n",
        "    val_error = np.mean((y_val - y_pred_val) ** 2)\n",
        "\n",
        "    # Agregar errores a las listas respectivas\n",
        "    train_errors.append(train_error)\n",
        "    val_errors.append(val_error)\n",
        "\n",
        "    # Guardar los errores para cada epoca\n",
        "    errors[epochs] = (np.mean(train_errors), np.mean(val_errors))\n",
        "\n",
        " # Imprimir errores promedio de enytrenamiento y validación en todas las épocas\n",
        "print(\"Deep NN: Errores promedio del entrenamiento y validación en todas las épocas:\")\n",
        "for epochs, (train_error, val_error) in errors.items():\n",
        "    print(f\"Epochs: {epochs}, Training MSE: {train_error:.2f}, Validation MSE: {val_error:.2f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "p_sT9NMp-q_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grafico de validacion del modelo**"
      ],
      "metadata": {
        "id": "mZMWO7RjDCAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los errores guardados en el diccionario \"errors\"\n",
        "epochs = list(errors.keys())\n",
        "train_errors = [error[0] for error in errors.values()]\n",
        "val_errors = [error[1] for error in errors.values()]\n",
        "\n",
        "# Crear la gráfica\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_errors, label='Error de entrenamiento (MSE)')\n",
        "plt.plot(epochs, val_errors, label='Error de validación (MSE)')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Error MSE')\n",
        "plt.title('Curvas de error de entrenamiento y validación')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Mostrar la gráfica\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wP_0xXWQDFgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones en el conjunto de datos de validación\n",
        "y_pred_val_all = pipeline.predict(X_val)\n",
        "\n",
        "# Escoge un subconjunto aleatorio de predicciones y objetivos\n",
        "subset_indices = np.random.choice(len(y_pred_val_all), size=len(y_pred_val_all), replace=False)\n",
        "y_pred_val_subset = y_pred_val_all[subset_indices]\n",
        "y_val_subset = y_val[subset_indices]\n",
        "\n",
        "# Graficar predicciones vs. objetivos\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_val_subset, y_pred_val_subset, color='blue', alpha=0.5)\n",
        "plt.plot([min(y_val_subset), max(y_val_subset)], [min(y_val_subset), max(y_val_subset)], color='red')\n",
        "plt.title('Predicción vs. Objetivo en el conjunto de datos de validación')\n",
        "plt.xlabel('Objetivo')\n",
        "plt.ylabel('Predicción')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l159KDWVD5tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define una función para graficar las curvas de aprendizaje\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Plots learning curves for a given estimator.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training error\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation error\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "# Usando la función para graficar las curvas de aprendizaje\n",
        "title = \"Curva de aprendizaje\"\n",
        "estimator = pipeline\n",
        "plot_learning_curve(estimator, title, X, y, ylim=(0, 5000), cv=kfold, n_jobs=-1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24mRxE8-E_At"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}